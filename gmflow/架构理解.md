### 个人认为的改进点

cnn的第一次下采样卷积核 虽然都是7*7 我是否可以换？
绝对位置编码转换为相对位置编码？但是跳跃连接只是会在cnn中，这样改进可能也没啥用？

### 架构入口在main函数的 `modlue` 里
```python
    model = GMFlow(feature_channels=args.feature_channels,
                   num_scales=args.num_scales,
                   upsample_factor=args.upsample_factor,
                   num_head=args.num_head,
                   attention_type=args.attention_type,
                   ffn_dim_expansion=args.ffn_dim_expansion,
                   num_transformer_layers=args.num_transformer_layers,
                   ).to(device)
```
--feature_channels', default=128, type=int
--num_scales', default=1, type=int, help= '1表示了1/8'
--upsample_factor', default=8, type=int
--num_head', default=1, type=int
--attention_type', default='swin', type=str
'--ffn_dim_expansion', default=4, type=int
--num_transformer_layers', default=6, type=int

### 逻辑运行全在forward中

第一步对输入图片对的处理， 送入extract_feature

extract_feature 的处理是送入  CNNEncoder
在这里得到的是1/8 的特征图 ，这里没有多维特征，而extract_feature中提供的多维特征提取方式是冗余内容
features 是一个只包含一个元素的列表。这个唯一的元素是一个四维张量，形状为 [2*B, 128, H/8, W/8]
并把两个batch的特征分块分成feature 0 和 1

第二步对特征加入位置编码，在这里使用的是detr的绝对位置编码，很聪明的选择了meta的实现
这里主要是有个窗口的设置，我不知道它运行过程中实际上是怎样执行的,但从调用情况来看并没有执行这个窗口的位置编码情况
现在更推荐相对位置编码

第三步进transformer
```python
        #--num_transformer_layers', default=6, type=int
        # Transformer
        self.transformer = FeatureTransformer(num_layers=num_transformer_layers,
                                              d_model=feature_channels,
                                              nhead=num_head,
                                              attention_type=attention_type,
                                              ffn_dim_expansion=ffn_dim_expansion,
                                              )
        # Transformer attn_splits_list=None
            feature0, feature1 = self.transformer(feature0, feature1, attn_num_splits=attn_splits)


```


### 对这个代码的swin transformer 完全解读， 这里是对第三步 中transformer代码的解释

从swin transformer 的实现逻辑来看，将代码分为如下几个部分：
打成patch 更改维度 加入位置编码 算窗口自注意力 移动窗口 算窗口自注意力 窗口恢复 patchmerging 相当于下采样 再更改通道数
进入算窗口自注意力 移动窗口 算窗口自注意力 窗口恢复 patchmerging 这样的循环
直到由特征图接上对应任务头

但这里的代码有不少的改动
从入口开始看，这样大体逻辑是清楚的，逐层调用

```python
class FeatureTransformer(nn.Module):
    def __init__(self,
                 num_layers=6,
                 d_model=128,
                 nhead=1,
                 attention_type='swin',
                 ffn_dim_expansion=4,
                 **kwargs,
                 ):
        super(FeatureTransformer, self).__init__()
        self.attention_type = attention_type
        self.d_model = d_model
        self.nhead = nhead
        #↑ 这些是参数自定义
        #↓ 看到了 nn.ModuleList 要注意数据量的定义
        self.layers = nn.ModuleList([
            TransformerBlock(d_model=d_model,
                             nhead=nhead,
                             attention_type=attention_type,
                             ffn_dim_expansion=ffn_dim_expansion,
                             with_shift=True if attention_type == 'swin' and i % 2 == 1 else False,
                             )
            for i in range(num_layers)])

        for p in self.parameters():
            # 如果参数的维度 > 1 (通常意味着它是权重矩阵，而不是偏置向量)
            if p.dim() > 1:
                # 使用 Xavier 均匀分布来初始化这个参数
                nn.init.xavier_uniform_(p)

    def forward(self, feature0, feature1,
                attn_num_splits=None,
                **kwargs,
                ):

        # ...
```

```python
# `with_shift` 满足了条件才为真，否则为 else 的值
with_shift = True if attention_type == 'swin' and i % 2 == 1 else False
```
`i` 是 `for i in range(num_layers)])` 这里的内容

`for p in self.parameters():`
遍历 FeatureTransformer 模块中的每一个参数
self.parameters() 是 PyTorch `nn.Module` 自带的一个方法，它会返回一个迭代器，其中包含了该模块（FeatureTransformer）以及它所有子模块（如 TransformerBlock, nn.Linear, nn.LayerNorm 等）中所有需要学习的参数。
p 在每次循环中，就是一个具体的参数张量，比如 self.layers[0].self_attn.q_proj.weight 或者 self.layers[0].self_attn.q_proj.bias。
if p.dim() > 1:
`p.dim()` 会返回参数张量 p 的维度数量。
这是一个过滤器，用来区分权重（Weight）和偏置（Bias）。
权重张量：通常是矩阵或更高维的张量。例如，一个 nn.Linear 层的权重形状是 [out_features, in_features]，维度是2。p.dim() > 1 为 True。
偏置张量：通常是向量。例如，一个 nn.Linear 层的偏置形状是 [out_features]，维度是1。p.dim() > 1 为 False。
**所以，这行代码的逻辑是：“我们只对权重进行特殊初始化，而跳过偏置。”**
为什么跳过偏置？ 偏置通常被初始化为0，因为它们只是对输出进行平移，不存在像权重那样导致信号逐层放大或缩小的问题。
nn.init.xavier_uniform_(p)
这是实际执行初始化的函数。函数名末尾的下划线 _ 表示这是一个**in-place（原地）**操作，它会直接修改张量 p 的值。
Xavier (也称 Glorot) 初始化是一种非常著名且有效的权重初始化方法。
```python
class FeatureTransformer(nn.Module):
    def __init__(self,
                 num_layers=6,
                 d_model=128,
                 nhead=1,
                 attention_type='swin',
                 ffn_dim_expansion=4,
                 **kwargs,
                 ):
        super(FeatureTransformer, self).__init__()
    #...

    def forward(self, feature0, feature1,
                attn_num_splits=None,
                **kwargs,
                ):

        b, c, h, w = feature0.shape
        assert self.d_model == c
        #  [B, C, H, W] → [B, H*W, C]
        feature0 = feature0.flatten(-2).permute(0, 2, 1)  # [B, H*W, C]
        feature1 = feature1.flatten(-2).permute(0, 2, 1)  # [B, H*W, C]
        #~ 计算 swin transformer 的掩码
        if self.attention_type == 'swin' and attn_num_splits > 1:
            # global and refine use different number of splits
            window_size_h = h // attn_num_splits
            window_size_w = w // attn_num_splits

            # compute attn mask once
            shifted_window_attn_mask = generate_shift_window_attn_mask(
                input_resolution=(h, w),
                window_size_h=window_size_h,
                window_size_w=window_size_w,
                shift_size_h=window_size_h // 2,
                shift_size_w=window_size_w // 2,
                device=feature0.device,
            )  # [K*K, H/K*W/K, H/K*W/K]
        else:
            shifted_window_attn_mask = None

        # concat feature0 and feature1 in batch dimension to compute in parallel
        concat0 = torch.cat((feature0, feature1), dim=0)  # [2B, H*W, C]
        concat1 = torch.cat((feature1, feature0), dim=0)  # [2B, H*W, C]

        for layer in self.layers:
            concat0 = layer(concat0, concat1,
                            height=h,
                            width=w,
                            shifted_window_attn_mask=shifted_window_attn_mask,
                            attn_num_splits=attn_num_splits,
                            )

            # update feature1
            concat1 = torch.cat(concat0.chunk(chunks=2, dim=0)[::-1], dim=0)

        feature0, feature1 = concat0.chunk(chunks=2, dim=0)  # [B, H*W, C]

        # reshape back
        feature0 = feature0.view(b, h, w, c).permute(0, 3, 1, 2).contiguous()  # [B, C, H, W]
        feature1 = feature1.view(b, h, w, c).permute(0, 3, 1, 2).contiguous()  # [B, C, H, W]

        return feature0, feature1
```


```python
        # concat feature0 and feature1 in batch dimension to compute in parallel
        concat0 = torch.cat((feature0, feature1), dim=0)  # [2B, H*W, C]
        concat1 = torch.cat((feature1, feature0), dim=0)  # [2B, H*W, C]

```
并行计算技巧？

### 计算光流的核心细节
```python
def global_correlation_softmax(feature0, feature1,
                               pred_bidir_flow=False,
                               ):
    # global correlation
    b, c, h, w = feature0.shape
    feature0 = feature0.view(b, c, -1).permute(0, 2, 1)  # [B, H*W, C]
    feature1 = feature1.view(b, c, -1)  # [B, C, H*W]
    
    correlation = torch.matmul(feature0, feature1).view(b, h, w, h, w) / (c ** 0.5)  # [B, H, W, H, W]

    # flow from softmax
    init_grid = coords_grid(b, h, w).to(correlation.device)  # [B, 2, H, W]
    grid = init_grid.view(b, 2, -1).permute(0, 2, 1)  # [B, H*W, 2]

    correlation = correlation.view(b, h * w, h * w)  # [B, H*W, H*W]

    if pred_bidir_flow:
        correlation = torch.cat((correlation, correlation.permute(0, 2, 1)), dim=0)  # [2*B, H*W, H*W]
        init_grid = init_grid.repeat(2, 1, 1, 1)  # [2*B, 2, H, W]
        grid = grid.repeat(2, 1, 1)  # [2*B, H*W, 2]
        b = b * 2

    prob = F.softmax(correlation, dim=-1)  # [B, H*W, H*W]

    correspondence = torch.matmul(prob, grid).view(b, h, w, 2).permute(0, 3, 1, 2)  # [B, 2, H, W]

    # when predicting bidirectional flow, flow is the concatenation of forward flow and backward flow
    flow = correspondence - init_grid

    return flow, prob

```
从这个函数直接得到预测光流，这个就是光流头，不过是按照1/8的特征进行的预测
**注意预测的细节**
- 由两个特征变换顺序计算`torch.matmul`来得到两个特征的相似性
- 得到初始网格坐标后（具体实现并没看）
- 计算特征内部的相似性 `softmax` 得到的相似性是概率，是为了加权求和的**权**
- 进行加权求和`torch.matmul` 得到预测的(x^,y^)
- 预测坐标减去初始坐标就是预测光流
